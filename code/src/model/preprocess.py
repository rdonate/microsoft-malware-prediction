#! - * - coding UTF-8 - * -

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import numpy as np
from src.model.select_features import selectFeatures
import os, gc
from joblib import load, dump

def preprocess(prefix_collections_train, prefix_collections_test,dir_SourceData, db, names_collection, types_of_features):
    dir_data = dir_SourceData + '/data/'
    if not os.path.exists(dir_data):
        os.mkdir(dir_data)
    tfidf = TfidfVectorizer(lowercase=True, token_pattern=r',', analyzer='word')
    features_txt_train = selectFeatures(db, prefix_collections_train, names_collection, ['text_without_name_feature'], 'str')
    with open(dir_data + 'pre_' + prefix_collections_train + types_of_features[1], 'wb') as file:
        dump(features_txt_train, file)
    file.close()
    tfidf.fit(features_txt_train)
    del features_txt_train
    gc.collect()
    with open(dir_data + 'tfid', 'wb') as file:
        dump(tfidf,file)
    file.close()
    gc.collect()
    with open(dir_data + 'tfid', 'rb') as file:
        tfidf = load(file)
    file.close()
    with open(dir_data + 'pre_' + prefix_collections_train + types_of_features[1], 'rb') as file:
        features_txt_train = load(file)
    file.close()
    tfidf_data = tfidf.transform(features_txt_train)
    del features_txt_train, tfidf
    gc.collect()
    with open(dir_data + prefix_collections_train + '_' + types_of_features[1], 'wb') as file:
        dump(tfidf_data, file)
    file.close()
    del file, tfidf_data
    gc.collect()
    features_txt_test = selectFeatures(db, prefix_collections_test, names_collection, ['text_without_name_feature'], 'str')
    with open(dir_data + 'tfid', 'rb') as file:
        tfidf = load(file)
    file.close()
    file.close()
    tfidf_data = tfidf.transform(features_txt_test)
    del features_txt_test, tfidf
    gc.collect()
    with open(dir_data + prefix_collections_test + '_' + types_of_features[1], 'wb') as file:
        dump(tfidf_data, file)
    file.close()
    del file
    gc.collect()
    label_train = selectFeatures(db, prefix_collections_train, names_collection, ['HasDetections'], np.int8)
    label_train = np.reshape(label_train, label_train.shape[0])
    with open(dir_SourceData + '/data/' + prefix_collections_train + '_' + types_of_features[-1], 'wb') as file:
        dump(label_train, file)
    file.close()
    del file, label_train
    gc.collect()
    with open(dir_SourceData + '/names_features_type/isNumerics' + prefix_collections_train, 'rb') as file:
        featuresNames = load(file)
    featuresNames.remove('HasDetections')
    featuresNames.remove('index')
    features_numeric_train = selectFeatures(db, prefix_collections_train, names_collection, featuresNames)
    with open(dir_SourceData + '/data/' + prefix_collections_train + '_' + types_of_features[0], 'wb') as file:
        dump(features_numeric_train, file)
    file.close()
    del file, features_numeric_train
    gc.collect()
    features_numeric_test = selectFeatures(db, prefix_collections_test, names_collection, featuresNames)
    with open(dir_SourceData + '/data/' + prefix_collections_test + '_' + types_of_features[0], 'wb') as file:
        dump(features_numeric_test, file)
    file.close()
    del file, features_numeric_test, featuresNames
    gc.collect()
